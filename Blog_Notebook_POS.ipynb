{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "590mqVpHk-90"
   },
   "source": [
    "# New Text-Fabric module: The Dead Sea Scrolls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGjIZtTVk-90"
   },
   "source": [
    "By Martijn Naaijer and Jarod Jacobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lUKwXYwk-91"
   },
   "source": [
    "Earlier this year, the CACCHT project (Creating Annotated Corpora of Classical Hebrew Texts) was started. CACCHT is a joint project of the ETCBC and the Theological Seminary at Andrews University and the researchers involved include: Jarod Jacobs, Martijn Naaijer, Robert Rezetko, Oliver Glanz and Wido van Peursen. CACCHT focuses on statistically analyzing Ancient Hebrew texts. At the core of our work is the BHSA and the extrabiblical module, but for a comprehensive analysis we intend to broaden our scope by including the Dead Sea Scrolls and Rabbinic texts.\n",
    "\n",
    "We have complete the first stage the project, the results of which can be found on the [ETCBC github page](https://github.com/ETCBC/dss): a brand new Text-Fabric module containing the Dead Sea Scrolls with morphological encoding.\n",
    "\n",
    "The DSS transcriptions and morphological data connected with them were generously provided by Martin Abegg. The transcriptions come from various sources, but primarily reflect what is found in the Discoveries in the Judean Desert series. Abegg started morphologically tagging the Qumran texts in the mid-90s with the assistance of several people. Over the following decades, Abegg completed full morphological tagging of nearly every Hebrew and Aramaic scroll found in the Judaean Desert between 1947 and today. In support of open access ideals, Abegg provided CACCHT his work from the past decades, which have been converted to Text-Fabric by Dirk Roorda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBrGcwKyk-92"
   },
   "source": [
    "## Part of Speech tagging of Hebrew texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RV2yCGClk-92"
   },
   "source": [
    "With Abegg's data in Text-Fabric, the next step is to convert Abegg's morphological encoding to the encoding system used by the ETCBC. Over the next few months we will work on converting word, phrase and clause features. For part of the data this is pretty straightforward. We assume that features like verb tense, stem formation, gender, number and state are similar to the ETCBC encoding, with only small adaptations needing to be made. Our initial foray into this will be converting part of speech tagging. The Abegg's dataset contains part of speech values, but its conventions deviate from that which is use in the ETCBC database.\n",
    "\n",
    "POS tagging of the Dead Sea Scroll has various challenges. In the first place, there are many ambiguous cases. For instance, the word אל can be a preposition, but it can also also be a noun meaning “god”. Of course a decision can be made by manually encoding all the DSS in the ETCBC encoding, or we can rely on POS tags and or other indirect information in the dataset of Abegg. The disadvantage of using other information from the dataset is that the conversion would become pretty complicated and in many cases the encoding would remain difficult. For instance, Abegg does not distinguish between part of speech (feature sp) and phrase dependent part of speech (feature pdp).\n",
    "\n",
    "In this project we want to tag the DSS with the ETCBC encoding system automatically, without manually encoding the logic behind each tag and decision.\n",
    "\n",
    "How does this work? First, it is important to state that we do not have clause boundaries in the Abegg dataset. This makes the task of POS tagging more difficult, because the structure of a clause may give an indication of the POS of a word. As an example, if a clause ends with the word אל, it is more likely to be a noun than a preposition, because a preposition is followed within the clause by other words or a pronominal suffix.\n",
    "\n",
    "Even with that limitation, Abegg's dataset does have information about the structure of words and their environment. Significantly, we know where word boundaries are, for instance, ויהי has been split into ו and יהי already. Also, POS tagging is helped by word morphology and, most importantly, we know the order of words in a book or text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WKPKYUbk-93"
   },
   "source": [
    "Automated systems for the analysis of language can be roughly divided into two kinds: rule-driven and pattern-driven. Rule-driven systems contain a lot of human input, such as \"if then\" blocks of code. For instance, in the case of a POS tagger, such a block can be: \"if a word is 'H', the POS is 'article'\", or \"if a word is 'MCH' or 'YHWH', the POS is 'proper noun'\". In general, this kind of systems works well, but there are some problems. One is that there are many ambiguous cases, and a rule-driven system can become very complicated to distinguish all the possible cases. Also, there may be patterns in the dataset that the researcher has missed, in which case the rule driven system remains incomplete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWEkwAP1k-94"
   },
   "source": [
    "In the CACCHT project we opt for the pattern-driven approach based on machine learning. Instead of relying on a system based on rules, we let an algorithm search for patterns in the data. In recent years, pattern-driven systems have started to outperform rule-driven system. Modern pattern-driven systems generally rely on machine learning algorithms to identify the structure of the data. The model is feed a large set of examples called the training set. For a POS tagging model, the training set contains words all tagged with their part of speech. The model identifies patterns in the training data from which it builds a structure that can be used tagged new texts that do not have part of speech tags. This approach is called supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvyD04NVk-94"
   },
   "source": [
    "For the CACCHT project, we train our model on the BHSA, where we know the POS of all the words. The model learns the relationship between words and the corresponding pos values, and then we use this model to predict the POS of the words in the DSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Vx2JmeOk-95"
   },
   "source": [
    "We have already seen that there are ambiguous cases, so how do we solve these? If it is possible to use the context of a word we would be helped enormously, because we expect that the preposition אל has a different environment in clause than the noun אל."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6PrS1xHk-95"
   },
   "source": [
    "To solve this problem, we use a so called sequence to sequence model (seq2seq). Instead of modeling the relationship between a word and a POS, we model the relationship between two sequences. One sequence consists of a number of words, the other of the corresponding POS. These sequences need to be kept relatively short, so we use a clause in each data sample which is a natural choice. However, as we have already mentioned, the Abegg data contain word boundaries, but there are no clause boundaries. Therefore we have chosen to feed the algorithm sequences of eight words. Here is an example of how this works:\n",
    "\n",
    "In ETCBC transcription, the first sequence in Genesis 1:1 looks as follows:\n",
    "\n",
    "'B R>CJT BR> >LHJM >T H CMJM W'\n",
    "\n",
    "This is the first input sequence. The corresponding output is a list and looks as follows.\n",
    "\n",
    "['\\t', 'prep', 'subs', 'verb', 'subs', 'prep', 'art', 'subs', 'conj', '\\n']\n",
    "\n",
    "The signs '\\t' and '\\n' are start and stop signs, occurring in every output sequence.\n",
    "\n",
    "For the second and third sample in the train set we move one word forward every time, so the inputs look as follows:\n",
    "\n",
    "'R>CJT BR> >LHJM >T H CMJM W >T'\n",
    "\n",
    "'BR> >LHJM >T H CMJM W >T H'\n",
    "\n",
    "The corresponding outputs are:\n",
    "\n",
    "['\\t', 'subs', 'verb', 'subs', 'prep', 'art', 'subs', 'conj', 'prep', '\\n']\n",
    "\n",
    "['\\t', 'verb', 'subs', 'prep', 'art', 'subs', 'conj', 'prep', 'art', '\\n']\n",
    "\n",
    "We move forward this way to the end of the book of Genesis, then we process Exodus and move forward until the end of Chronicles is reached. One book is withheld from the model (in our case Nehemiah), to act as the test set. Keeping part of the data separate as a test set is a standard procedure in machine learning practice.\n",
    "\n",
    "You can see the seq2seq model as a translation model, and this is exactly what it is used for in other applications. In these applications the input can consist of English sentences, which are translated by the model to another language, for instance Dutch.\n",
    "\n",
    "What kind of algorithms can be used for such a task, in which a sequence of POS is predicted for a sequence of characters? A type of model which is used often for sequence analysis is the so-called Long Short-Term Memory model (or LSTM model), which is a kind of Neural Network. It is used for a variety of Natural Language Processing tasks (such as chatbots, text classification and text summarization), but also for making predictions of numeric sequences, such as forecasting time-series. It is beyond the scope of this blog to go into the details of Neural Networks and the LSTM model, but there are a lot of helpful sources  online about it, such as [this blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) or the [Keras documentation of seq2seq LSTM models](https://keras.io/examples/lstm_seq2seq/).\n",
    "\n",
    "One challenge with LSTM models is that the algorithm only ingests numbers and all sequences have to have the same length. Because of this, some further preprocessing is needed. We check the length of the longest sequence and give all the sequences that length by adding zeros to it (this is called padding). All sequences consist of eight words, so how can they have varying lengths? We have chosen to use a character based model, so the model sees the input sequence as a sequence of characters. It does not know where the word boundaries are, because the model takes the space as a character just like the other characters. We also convert each character to a number so that the model can work with them.\n",
    "\n",
    "Then all those input and output sequences are fed to the algorithm, which trains a model that finds the relationship between the input and output sequences (at least, that is what we hope, of course).\n",
    "\n",
    "With this model and the input sequences of the DSS we can predict their POS, but how do we know how well the model performs? Predictions based on machine learning models rarely predict everything correctly. To find out how good it is, we start with making predictions on the test set: the book of Nehemiah. We make predictions on the words (first converted to sequences similar to the training data), and then compare these predictions with the true values of the POS in the ETCBC database. When that is done we know how often it predicts unseen words correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQ7v3nHHk-96"
   },
   "source": [
    "## Let's do some real work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZS5bEKkk-96"
   },
   "source": [
    "The following script works through this whole procedure of training the LSTM model and makes predictions on the test set. The following steps are made:\n",
    "\n",
    "- import of relevant libraries\n",
    "- prepare_train_data() in this function input and output sequences for the train set are created.\n",
    "- prepare test data() input and output sequences for the test set are created.\n",
    "- in create_dicts() and one_hot_encode() the sequences are preprocessed further.\n",
    "- in define_LSTM_model() the encode-decode architecture is created.\n",
    "- compile_and_train() does the training of the model. Here some important hyperparameters of the model are chosen.\n",
    "- After that, predictions are made using the model and the test set, which is the book of Nehemiah. After the evaluation it becomes clear how well the model works on unseen data. These predictions demonstrate what we want to use the model for: automatically analyzing Hebrew texts grammatically!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geNCugY_k-97"
   },
   "source": [
    "First some libraries are imported. Of course, we use [Text-Fabric with the BHSA data](https://etcbc.github.io/bhsa) which you can access with Python 3 and as framework for the Neural Network we use [Keras](https://keras.io).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "colab_type": "code",
    "id": "jZ7KctzGlMyt",
    "outputId": "3cc0172f-3a7e-4098-94ff-0dc46630d8cf"
   },
   "outputs": [],
   "source": [
    "!pip install text-fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ivWu1MRsk-98",
    "outputId": "ef75e70d-ca0c-45ca-b7db-36d0c575349b"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from statistics import mode\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "colab_type": "code",
    "id": "aNt3lEUMk-9-",
    "outputId": "4e514188-54b5-4d09-b1a1-e3f4de61f934"
   },
   "outputs": [],
   "source": [
    "from tf.app import use\n",
    "A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywpLAZGfk--A"
   },
   "source": [
    "In the function prepare_train_data() the train set is created, and some other useful information is collected. The argument of the function, test_book, is the book which will be excluded from the train set, because it is upon this book that the model will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NxXowWkk--A"
   },
   "outputs": [],
   "source": [
    "def prepare_train_data(test_book):\n",
    "\n",
    "    input_seqs = []\n",
    "    output_pos = []\n",
    "    input_chars = set()\n",
    "    output_vocab = set()\n",
    "\n",
    "    # iterate over all the books\n",
    "    for bo in F.otype.s(\"book\"): \n",
    "        \n",
    "        # exclude the test_book\n",
    "        if F.book.v(bo) == test_book:\n",
    "            continue\n",
    "               \n",
    "        # all the words from a book are collected\n",
    "        words = L.d(bo, 'word')\n",
    "        \n",
    "        # Now we iterate over all the words, except the last words, because all the sequences have to be 8 words long\n",
    "        for w in words[0:-7]:\n",
    "            \n",
    "            # In the following two lines the train data are prepared\n",
    "            \n",
    "            #exclude Aramaic words\n",
    "            languages_list = [F.language.v(w) for w in range(w, w+8) if (F.g_cons.v(w) != '')]\n",
    "            if \"Aramaic\" in languages_list:\n",
    "                continue\n",
    "            \n",
    "            # here the input data are created\n",
    "            # words_train is a string with the consonantal representation of 8 words, separated by spaces\n",
    "            # elided-he is excluded, this is the empty string\n",
    "            g_cons_train = (\" \".join([F.g_cons.v(w) for w in range(w, w+8) if (F.g_cons.v(w) != '')])).strip()\n",
    "            \n",
    "            # and here outputs are created\n",
    "            # it is a list containing parts of speech\n",
    "            parts_of_speech = [F.sp.v(w) for w in range(w, w+8) if (F.g_cons.v(w) != '')]\n",
    "            \n",
    "            # the two preceding lines of code and their counterparts in the function prepare_test_data() are the only places \n",
    "            # where we extract data from the etcbc database with text-fabric before the data are trained\n",
    "            \n",
    "            # to the outputs a start ('\\t') and stop ('\\n') symbol are added\n",
    "            parts_of_speech = ['\\t'] + parts_of_speech + ['\\n']\n",
    "             \n",
    "            # the input sepuence g_cons_train is added to input_seqs, which is a list containing all the inputs\n",
    "            input_seqs.append(g_cons_train)\n",
    "            \n",
    "            # the list parts_of_speech is added to the list output_pos\n",
    "            output_pos.append(parts_of_speech)\n",
    "            \n",
    "            # for further processing we need the \"vocabularies\" of the input and output\n",
    "            # we use a character-based model, so the input vocabulary consists of all the distinct characters in the input strings\n",
    "            # also included is the space\n",
    "            for ch in g_cons_train:\n",
    "                input_chars.add(ch)\n",
    "            \n",
    "            # also collected is the output vocabulary, which consists of all the parts of speech in the etcbc database\n",
    "            for pos in parts_of_speech:\n",
    "                output_vocab.add(pos)\n",
    "                \n",
    "    \n",
    "    input_chars = sorted(list(input_chars))\n",
    "    output_vocab = sorted(list(output_vocab))\n",
    "    \n",
    "    # in the LSTM network all the sequences have to have the same length. We find out what the length of the longest sequence is,\n",
    "    # all the other sequences will get that length\n",
    "    max_len_input = max([len(clause) for clause in input_seqs])\n",
    "    max_len_output = max([len(poss) for poss in output_pos])\n",
    "    \n",
    "    # shuffle the data. The model will get the data in small batches, it is preferable if the batches are more or less homogeneous\n",
    "    # of course the inputs and outputs have to be shuffled identically\n",
    "    input_seqs, output_pos = shuffle(input_seqs, output_pos)\n",
    "    \n",
    "    return input_seqs, output_pos, input_chars, output_vocab, max_len_input, max_len_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-IMtqIvk--C"
   },
   "source": [
    "In the function prepare_test_data() the test data are prepared, consisting of the data of the single book not included in the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tG5fYKYJk--D"
   },
   "outputs": [],
   "source": [
    "def prepare_test_data(test_book):\n",
    "\n",
    "    input_seqs_test = []\n",
    "    output_seqs_test = []\n",
    "    g_cons_test = []\n",
    "    pos_test = [] \n",
    "    \n",
    "    for bo in F.otype.s('book'):\n",
    "        \n",
    "        # exclude other books than test_book\n",
    "        if F.book.v(bo) != test_book:\n",
    "            continue\n",
    "            \n",
    "        words = L.d(bo, 'word')\n",
    "\n",
    "        for w in words[0:-7]:\n",
    "          \n",
    "            # exclude Aramaic words\n",
    "            languages_list = [F.language.v(w) for w in range(w, w+8) if (F.g_cons.v(w) != '')]\n",
    "            if \"Aramaic\" in languages_list:\n",
    "                continue\n",
    "            \n",
    "            if F.g_cons.v(w) == '':\n",
    "                continue\n",
    "            \n",
    "            # prepare the test data\n",
    "            input_seq_test = (\" \".join([F.g_cons.v(w) for w in range(w, w+8) if (F.g_cons.v(w) != '')])).strip()\n",
    "            output_seq_test = [F.sp.v(w) for w in range(w, w+8) if (F.g_cons.v(w) != '')]\n",
    "            \n",
    "            input_seqs_test.append(input_seq_test)\n",
    "            output_seqs_test.append(output_seq_test)\n",
    "            \n",
    "    return input_seqs_test, output_seqs_test, [w for w in words if (F.g_cons.v(w) != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfApAXgqk--E"
   },
   "source": [
    "The network can only handle numeric data, but after the data have been processed as numbers, they need to be converted back to characters. The function create_dicts() provides dictionaries with mapping between the input and output vocabularies and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0GpacW1k--F"
   },
   "outputs": [],
   "source": [
    "def create_dicts(input_voc, output_voc):\n",
    "    \n",
    "    # these dicts map the input sequences\n",
    "    input_idx2char = {}\n",
    "    input_char2idx = {}\n",
    "\n",
    "    for k, v in enumerate(input_voc):\n",
    "        input_idx2char[k] = v\n",
    "        input_char2idx[v] = k\n",
    "     \n",
    "    # and these dicts map the output sequences of parts of speech\n",
    "    output_idx2char = {}\n",
    "    output_char2idx = {}\n",
    "    \n",
    "    for k, v in enumerate(output_voc):\n",
    "        output_idx2char[k] = v\n",
    "        output_char2idx[v] = k\n",
    "        \n",
    "    return input_idx2char, input_char2idx, output_idx2char, output_char2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zd_KpcCzk--G"
   },
   "source": [
    "Now the final data preparation function is made. Categorical data are generally fed to the LSTM network in one-hot encoded form. The inputs and the outputs have the same length. Also created is an array called decoder_target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdvEI1oxk--H"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(nb_samples, max_len_input, max_len_output, input_chars, output_vocab, input_char2idx, output_char2idx, input_clauses, output_pos):\n",
    "    \n",
    "    # three-dimensional numpy arrays are created \n",
    "    tokenized_input = np.zeros(shape = (nb_samples, max_len_input, len(input_chars)), dtype='float32')\n",
    "    tokenized_output = np.zeros(shape = (nb_samples, max_len_output, len(output_vocab)), dtype='float32')\n",
    "    target_data = np.zeros((nb_samples, max_len_output, len(output_vocab)), dtype='float32')\n",
    "\n",
    "    for i in range(nb_samples):\n",
    "        for k, ch in enumerate(input_clauses[i]):\n",
    "            tokenized_input[i, k, input_char2idx[ch]] = 1\n",
    "        \n",
    "        for k, ch in enumerate(output_pos[i]):\n",
    "            tokenized_output[i, k, output_char2idx[ch]] = 1\n",
    "\n",
    "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "            if k > 0:\n",
    "                target_data[i, k-1, output_char2idx[ch]] = 1\n",
    "                \n",
    "    return tokenized_input, tokenized_output, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glFWVabBk--J"
   },
   "source": [
    "In the function define_LSTM_model() the architecture of the model is created. Neural networks are very flexible structures and a variety of architectures have been developed for various tasks. Here we use the encoder-decoder architecture with two LSTM layers in the encoder. In the architecture there is a variety of hyperparameters that you have to choose. Better hyperparameters lead to better predictions, so it is important to spend time on optimizing this. Hyperparameters in this architecture are the number of LSTM layers, the number of cells in each LSTM layer and the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3zy0AR1k--J"
   },
   "outputs": [],
   "source": [
    "def define_LSTM_model(input_chars, output_vocab):\n",
    "\n",
    "    # encoder model\n",
    "    encoder_input = Input(shape=(None,len(input_chars)))\n",
    "    encoder_LSTM = LSTM(250,activation='relu',return_state=True, return_sequences=True)(encoder_input)\n",
    "    encoder_LSTM = LSTM(250,return_state=True)(encoder_LSTM)\n",
    "    encoder_outputs, encoder_h, encoder_c = encoder_LSTM\n",
    "    encoder_states = [encoder_h, encoder_c]\n",
    "    \n",
    "    # decoder model\n",
    "    decoder_input = Input(shape=(None,len(output_vocab)))\n",
    "    decoder_LSTM = LSTM(250, return_sequences=True, return_state = True)\n",
    "    decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(len(output_vocab), activation='softmax')\n",
    "    decoder_out = decoder_dense (decoder_out)\n",
    "    \n",
    "    model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return encoder_input, encoder_states, decoder_input, decoder_LSTM, decoder_dense, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hykatv5nk--L"
   },
   "source": [
    "Now the model is compiled and trained using the function compile_and_train(). The data are fed to the model in small batches. The train data are split in a train and validation set. The latter data consist of 5% of the original train set. The model is trained on the train set, and makes a prediction on these data. The difference between the predictions and the true values of the output are calculated with categorical crossentropy and is called the loss. During training this loss becomes smaller, which means that the predictions become more accurate. However, we want the model not only to become good on the train data, but it should be general enough to make accurate predictions on unseen data. Therefore, after every epoch a prediction is made on the small validation set and the validation loss is calculated. Ideally, the validation loss is more or less equal to the train loss. After a number of epochs, you will notice that the train loss keeps decreasing, while the validation loss remains equal or even increases. At this point the model starts to overfit, which means that the algorithm is modeling idiosyncrasies in the train data instead of general patterns. In that case it is time to stop training and make predictions on the test set.\n",
    "\n",
    "Again, you have to choose a number of hyperparameters. These are the optimizer, the loss function, the batch size, the number of epochs and the learning rate. If you want, you can even tune more hyperparameters.\n",
    "\n",
    "With Earlystopping() the training process can be stopped earlier than the given number of epochs. This is useful if the model starts overfitting and the validation loss does not decrease anymore.\n",
    "\n",
    "Note that training an LSTM model is a computationally intensive process. It is recommended to run the script on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDBYm_J5k--L"
   },
   "outputs": [],
   "source": [
    "def compile_and_train(model, one_hot_in, one_hot_out, targets, batch_size, epochs, val_split):\n",
    "\n",
    "    callback = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "    adam = Adam(lr=0.0008, beta_1=0.99, beta_2=0.999, epsilon=0.00000001)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "    model.fit(x=[one_hot_in,one_hot_out], \n",
    "              y=targets,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=val_split,\n",
    "              callbacks=[callback])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsOt8RhYk--N"
   },
   "source": [
    "The train data are prepared. The test data consist of sequences of words from the book of Nehemiah, so in the preparation of the train data, Nehemiah is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRZvKIkxk--N"
   },
   "outputs": [],
   "source": [
    "test_book = \"Nehemia\" # the book name is in Latin, because the tf-feature \"book\" is used in the functions prepare_train_data() and prepare_test_data().\n",
    "\n",
    "input_clauses, output_pos, input_chars, output_vocab, max_len_input, max_len_output = prepare_train_data(test_book)\n",
    "input_idx2char, input_char2idx, output_idx2char, output_char2idx = create_dicts(input_chars, output_vocab)\n",
    "\n",
    "nb_samples = len(input_clauses)\n",
    "one_hot_input, one_hot_output, target_data = one_hot_encode(nb_samples, max_len_input, max_len_output, input_chars, output_vocab, input_char2idx, output_char2idx, input_clauses, output_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPBJtkntmn-u"
   },
   "source": [
    "What do the input data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "SEWMw_LZmtEa",
    "outputId": "a6f9738f-2e4a-4659-9201-fab4256319da"
   },
   "outputs": [],
   "source": [
    "input_clauses[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "fbbzqC4GmzrG",
    "outputId": "2ddab093-d9c0-4ca5-ac78-a32649140f95"
   },
   "outputs": [],
   "source": [
    "output_pos[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4dNIglsk--Q"
   },
   "source": [
    "The test data are prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YghmMRcZk--Q"
   },
   "outputs": [],
   "source": [
    "test_clauses, output_test, test_word_nodes = prepare_test_data(test_book)\n",
    "one_hot_test_data, _, _ = one_hot_encode(len(test_clauses), max_len_input, max_len_output, input_chars, output_vocab, input_char2idx, output_char2idx, test_clauses, output_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R5lrsxHCk--S"
   },
   "source": [
    "Here the functions define_LSTM_model() and compile_and_train() are called. A neural network learns in an iterative process. One iteration is called an epoch. In each iteration a prediction is made, and the train and validation loss are calculated, as you can see in the output.\n",
    "\n",
    "The architecture of the model is also printed with the number of parameters. You also see the number of train samples (397552 samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AzgZ838jk--T",
    "outputId": "d25eaad3-26a5-4d45-da67-4c1099515b4f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_input, encoder_states, decoder_input, decoder_LSTM, decoder_dense, model = define_LSTM_model(input_chars, output_vocab)\n",
    "model = compile_and_train(model, one_hot_input, one_hot_output, target_data, 1024, 150, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FB3HfDSyk--U"
   },
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(250,))\n",
    "decoder_state_input_c = Input(shape=(250,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJorskGDk--X"
   },
   "source": [
    "In the function decode_seq() the predictions on the test set are made. The input, inp_seq, consists of one one-hot encoded sequence of words in the book of Nehemiah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-zKVmwjk--Y"
   },
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(output_vocab)))\n",
    "    target_seq[0, 0, output_char2idx['\\t']] = 1\n",
    "    \n",
    "    pred_pos = []\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_out_char = output_idx2char[max_val_index]\n",
    "        pred_pos.append(sampled_out_char)\n",
    "        \n",
    "        if (sampled_out_char == '\\n'):\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(output_vocab)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NSqLp0Lk--c"
   },
   "source": [
    "Now the function decode_seq() is called, the predictions are made and the results are preprocessed. \n",
    "\n",
    "For most words eight predictions are made, because each word (except the words at the beginning and end of a book) occurs in eight sequences. In the dict decision_dict all eight predictions are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ceL_8bXnk--c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decision_dict = collections.defaultdict(list)\n",
    "\n",
    "for seq_index in range(len(one_hot_test_data)):\n",
    "    inp_seq = one_hot_test_data[seq_index:seq_index+1]\n",
    "    \n",
    "    pred_pos = decode_seq(inp_seq)\n",
    "    \n",
    "    if len(pred_pos[:-1]) == len(output_test[seq_index]):\n",
    "        for pred_ind in range(len(pred_pos[:-1])):\n",
    "            decision_dict[seq_index + pred_ind].append(pred_pos[:-1][pred_ind])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibBeV9kUk--f"
   },
   "source": [
    "We simply use majority voting to decide what the final prediction is. So, if the model predicts 5 times \"verb\" and 3 times \"subs\" for a certain word, we decide that the word is a verb. In the case of a tie, e.g. 4 times \"verb\" and 4 times \"subs\", the value with the lowest index is chosen, which can be seen as a random choice from the alternatives with equal numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHf2cjwbk--f"
   },
   "source": [
    "### Misclassifications on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUQDQivwk--g"
   },
   "source": [
    "We start the evaluation with the bad news: misclassifications. We want the model to predict the POS correctly as often as possible, but in practice it is difficult to reach 100% accuracy. The following cell outputs the words in the book in Nehemiah that were misclassified by the model.\n",
    "\n",
    "The output shows:\n",
    "\n",
    "- the text-fabric node number\n",
    "- (book, chapter, verse)\n",
    "- the consonants of a word\n",
    "- correct POS\n",
    "- predicted POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "re8xQRAFk--g",
    "outputId": "ca3f75d4-7d61-45c2-9364-9b138be14155",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correct_test = 0\n",
    "wrong_test = 0\n",
    "cross_dict = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "\n",
    "for key in range(len(test_word_nodes)):\n",
    "    data = collections.Counter(decision_dict[key])\n",
    "    cross_dict[F.sp.v(test_word_nodes[key])][max(decision_dict[key], key=data.get)] += 1\n",
    "\n",
    "    if F.sp.v(test_word_nodes[key]) == max(decision_dict[key], key=data.get):\n",
    "        correct_test += 1\n",
    "\n",
    "    else:\n",
    "        wrong_test += 1\n",
    "        print(test_word_nodes[key], T.sectionFromNode(test_word_nodes[key]), F.g_cons.v(test_word_nodes[key]), F.sp.v(test_word_nodes[key]), max(decision_dict[key], key=data.get))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rcw50KQRk--i"
   },
   "source": [
    "### Quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0HxPya1k--i"
   },
   "source": [
    "The following table shows the predictions in the rows and the true values according to the ETCBC database in the columns. On the diagonal you see the numbers of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36NmWH4_k--j",
    "outputId": "70fb2bc0-da4a-461c-af83-6d9a8f0f145c"
   },
   "outputs": [],
   "source": [
    "evaluation = []\n",
    "\n",
    "all_pos = list(cross_dict.keys())\n",
    "\n",
    "for key in all_pos:\n",
    "    eval_pos = [cross_dict[key][key2] if key2 in cross_dict[key] else 0 for key2 in all_pos]\n",
    "    evaluation.append(eval_pos)\n",
    "    \n",
    "# put everything in dataframe\n",
    "df_eval = pd.DataFrame(evaluation) \n",
    "df_eval.columns = all_pos\n",
    "df_eval.index = all_pos\n",
    "df_eval\n",
    "\n",
    "# Below:\n",
    "# horizontal: predictions\n",
    "# vertical: true values according to etcbc database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1g5-aB4uk--k"
   },
   "source": [
    "All these result are interesting, but how good is the model? We calculate this by dividing the number of misclassifications by the total number of predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0N0XnAAk--l",
    "outputId": "4c4a48f9-9881-4d65-8dbe-a09b98b1b758"
   },
   "outputs": [],
   "source": [
    "print(\"Correct classifications:\", correct_test)\n",
    "print(\"Misclassifications:\", wrong_test)\n",
    "\n",
    "correct_percent = 100 * correct_test  / (correct_test + wrong_test)\n",
    "print(\"Accuracy:\", round(correct_percent, 1), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mhjkzO1k--n"
   },
   "source": [
    "So, the model predicts the POS of biblical data correctly in nearly 96% of the words, which we think is decent. The most difficult POS to predict is the proper noun (nmpr), in 93 cases in which the true value is a proper noun a substantive (subs) was predicted. The results may vary slightly between different runs of the script.\n",
    "\n",
    "The model can be saved and loaded again to be used for making predictions on for instance the Dead Sea Scrolls. The language of the DSS may differ a bit from Biblical Hebrew, which may lead to a slight decrease in accuracy, but on the other hand, the extra-biblical text-fabric module contains some DSS scrolls already, which is helpful, because they can already be added to the training set. This addition of other training data is only one way to improve the model. There might be various other ways. If you have suggestions, or if you are a student and you would like to do a project on Ancient Hebrew and machine learning, let us know!\n",
    "\n",
    "While it is difficult to say beforehand how the model can be improved and how good exactly the algorithm works on unseen DSS, we will update you soon with our findings in a another blogpost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qy4bdFDtk--n"
   },
   "source": [
    "### Correct classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aavuU-E1k--o"
   },
   "source": [
    "Finally, for the record, these are the correct predictions. In the output you see:\n",
    "\n",
    "- The text-fabric node number\n",
    "- (book, chapter, verse)\n",
    "- the consonants of the word\n",
    "- correct pos\n",
    "- predicted pos (which is identical to the correct pos, of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqE19zApk--o",
    "outputId": "1c86c3f7-7548-49f0-a0d1-dc66915f29da",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in range(len(test_word_nodes)):\n",
    "    data = collections.Counter(decision_dict[key])\n",
    "    cross_dict[F.sp.v(test_word_nodes[key])][max(decision_dict[key], key=data.get)] += 1\n",
    "\n",
    "    if F.sp.v(test_word_nodes[key]) == max(decision_dict[key], key=data.get):\n",
    "\n",
    "        print(test_word_nodes[key], T.sectionFromNode(test_word_nodes[key]), F.g_cons.v(test_word_nodes[key]), F.sp.v(test_word_nodes[key]), max(decision_dict[key], key=data.get))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Blog_Notebook_POS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
